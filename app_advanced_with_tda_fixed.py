# -*- coding: utf-8 -*-
"""
Advanced MS MRI Analysis Server with TDA - Optimized for Large Files

This Flask application provides an advanced API for analyzing brain MRI scans
for signs of Multiple Sclerosis (MS) using deep learning and topological data analysis (TDA).
Fully optimized for large file processing with streaming and incremental processing.
"""
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

import warnings
warnings.filterwarnings('ignore')

import numpy as np
from flask import Flask, request, jsonify, render_template
import tempfile
import base64
import io
from PIL import Image
import logging
import cv2
import nibabel as nib
import tensorflow as tf
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy import ndimage
from skimage import measure
import gdown
import gc
import time
import sys

# Ø­Ù„ Ø¨Ø¯ÙŠÙ„ Ù„Ù€ gtda Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¹Ù…Ù„
try:
    from gtda.homology import VietorisRipsPersistence
    TDA_AVAILABLE = True
except ImportError:
    TDA_AVAILABLE = False
    print("âš ï¸ Giotto-TDA not available, using geometric features only")

logging.getLogger('werkzeug').setLevel(logging.ERROR)

app = Flask(__name__)
app.config['JSON_SORT_KEYS'] = False

# ========== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ==========
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500MB

print("ğŸš€ Starting Advanced MS MRI Analysis Server with TDA (Optimized for Large Files)...")

# =====================================================
# Load AI Models
# =====================================================

print("Loading AI models...")

# Initialize models
unet_model = None
rf_model = None
scaler = None

def download_unet_model():
    """Download U-Net model from Google Drive"""
    model_path = "best_unet_final.keras"
    if not os.path.exists(model_path):
        print("ğŸ“¥ Downloading U-Net model from Google Drive...")
        try:
            url = "https://drive.google.com/uc?id=1CgugA_Ti0prkQH3j7NL_pEmXjZx-FfdB&confirm=t"
            gdown.download(url, model_path, quiet=False)
            
            if os.path.exists(model_path):
                file_size = os.path.getsize(model_path) / (1024*1024)
                print(f"âœ… U-Net model downloaded successfully ({file_size:.1f} MB)")
                return True
            else:
                print("âŒ File was not downloaded")
                return False
                
        except Exception as e:
            print(f"âŒ Failed to download U-Net model: {e}")
            return False
    return True

def dice_coefficient(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.cast(tf.keras.backend.flatten(y_true), "float32")
    y_pred_f = tf.keras.backend.flatten(y_pred)
    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)

def bce_dice_loss(y_true, y_pred):
    bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)
    return bce + (1 - dice_coefficient(y_true, y_pred))

try:
    if download_unet_model():
        custom_objects = {"dice_coefficient": dice_coefficient, "bce_dice_loss": bce_dice_loss}
        unet_model = tf.keras.models.load_model("best_unet_final.keras", custom_objects=custom_objects)
        print("âœ… U-Net model loaded successfully")
    else:
        print("âš ï¸ Using basic mode without U-Net")
except Exception as e:
    print(f"âš ï¸ U-Net model loading failed: {e}")

try:
    rf_model = joblib.load("rf_classifier.pkl")
    print("âœ… Random Forest model loaded successfully")
except Exception as e:
    print(f"âš ï¸ Random Forest model loading failed: {e}")

try:
    scaler = joblib.load("scaler.pkl")
    print("âœ… Scaler loaded successfully")
except Exception as e:
    print(f"âš ï¸ Scaler loading failed: {e}")

# =====================================================
# Large File Processing - Streamlined Approach
# =====================================================

def process_very_large_nii_file(file_path, target_slices=150):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªØ¯Ø±Ø¬Ø©"""
    print(f"ğŸ“ Processing VERY large NII file: {file_path}")
    
    try:
        # ÙØªØ­ Ø§Ù„Ù…Ù„Ù Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
        nii_img = nib.load(file_path)
        img_data = nii_img.get_fdata()
        original_shape = img_data.shape
        
        print(f"ğŸ“Š Original image dimensions: {original_shape}")
        print(f"ğŸ’¾ Estimated memory usage: {img_data.nbytes / (1024*1024*1024):.2f} GB")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        total_slices = img_data.shape[2]
        
        if total_slices > 300:
            # Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¶Ø®Ù…Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¹ÙŠÙ†Ø§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
            target_slices = min(120, total_slices)  # ØªÙ‚Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± Ù„Ù„Ø°Ø§ÙƒØ±Ø©
            
            # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº
            sample_indices = strategic_sampling(total_slices, target_slices)
            processed_data = img_data[:, :, sample_indices]
            
            print(f"ğŸ”§ Very large file: Processing {target_slices} strategic samples from {total_slices} slices")
            
        elif total_slices > 150:
            # Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¹ÙŠÙ†Ø§Øª Ù…ÙˆØ²Ø¹Ø©
            sample_indices = np.linspace(0, total_slices - 1, target_slices, dtype=int)
            processed_data = img_data[:, :, sample_indices]
            
            print(f"ğŸ”§ Large file: Processing {target_slices} distributed samples from {total_slices} slices")
            
        else:
            # Ù…Ù„Ù Ø¹Ø§Ø¯ÙŠ - Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØ§Ù…Ù„Ø©
            processed_data = img_data
            sample_indices = list(range(total_slices))
            
        print(f"ğŸ“Š Final dimensions: {processed_data.shape}")
        return processed_data, sample_indices, total_slices
        
    except Exception as e:
        print(f"âŒ Error processing large NII file: {e}")
        raise

def strategic_sampling(total_slices, target_slices):
    """Ø£Ø®Ø° Ø¹ÙŠÙ†Ø§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ù† Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø¯Ù…Ø§Øº"""
    # Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„ÙˆØ³Ø·Ù‰ Ø­ÙŠØ« ØªÙˆØ¬Ø¯ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø¢ÙØ§Øª
    middle_start = total_slices // 4
    middle_end = 3 * total_slices // 4
    
    # Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„ÙˆØ³Ø·Ù‰ (80%)
    middle_samples = int(target_slices * 0.8)
    middle_indices = np.linspace(middle_start, middle_end, middle_samples, dtype=int)
    
    # Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø·Ø±ÙÙŠØ© (20%)
    edge_samples = target_slices - middle_samples
    edge_indices1 = np.linspace(0, middle_start-1, edge_samples//2, dtype=int)
    edge_indices2 = np.linspace(middle_end+1, total_slices-1, edge_samples//2, dtype=int)
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª
    all_indices = np.concatenate([edge_indices1, middle_indices, edge_indices2])
    return np.sort(all_indices)

def incremental_preprocessing(img_data, batch_size=16):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ¯Ø±Ø¬Ø© Ù„Ù„Ø´Ø±Ø§Ø¦Ø­ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    print(f"ğŸ”§ Incremental preprocessing of {img_data.shape[2]} slices...")
    
    all_slices = []
    
    for i in range(0, img_data.shape[2], batch_size):
        batch_end = min(i + batch_size, img_data.shape[2])
        batch_slices = []
        
        for j in range(i, batch_end):
            try:
                # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø´Ø±ÙŠØ­Ø© Ø¹Ù„Ù‰ Ø­Ø¯Ø©
                sl = img_data[:, :, j]
                sl_normalized = (sl - np.min(sl)) / (np.max(sl) - np.min(sl) + 1e-8)
                sl_resized = cv2.resize(sl_normalized, (128, 128))
                sl_final = np.expand_dims(sl_resized, axis=-1)
                batch_slices.append(sl_final)
            except Exception as e:
                print(f"âš ï¸ Error processing slice {j}: {e}")
                # Ø¥Ù†Ø´Ø§Ø¡ Ø´Ø±ÙŠØ­Ø© ÙØ§Ø±ØºØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
                empty_slice = np.zeros((128, 128, 1))
                batch_slices.append(empty_slice)
        
        all_slices.extend(batch_slices)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¯ÙØ¹Ø©
        del batch_slices
        gc.collect()
        
        if (i // batch_size) % 10 == 0:
            print(f"   Processed {batch_end}/{img_data.shape[2]} slices")
    
    return np.array(all_slices)

def streaming_unet_segmentation(slices, threshold=0.1, batch_size=8):
    """ØªØ¬Ø²Ø¦Ø© U-Net Ù…ØªØ¯Ø±Ø¬Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    if unet_model is None:
        print("âš ï¸ U-Net model not available, using efficient mock segmentation")
        return create_efficient_mock_masks(slices)

    print("Running streaming U-Net segmentation...")
    
    all_masks = []
    total_batches = (len(slices) + batch_size - 1) // batch_size
    
    for i in range(0, len(slices), batch_size):
        batch_end = min(i + batch_size, len(slices))
        batch_slices = slices[i:batch_end]
        
        print(f"   Segmenting batch {i//batch_size + 1}/{total_batches}")
        
        try:
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙÙ‚Ø·
            batch_predictions = unet_model.predict(batch_slices, verbose=0)
            batch_masks = (batch_predictions > threshold).astype(np.uint8)
            all_masks.extend(batch_masks)
            
            # ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„ Ù„Ù„Ø°Ø§ÙƒØ±Ø©
            del batch_slices, batch_predictions, batch_masks
            gc.collect()
            
        except Exception as e:
            print(f"âš ï¸ Error in batch {i//batch_size + 1}: {e}")
            # Ø¥Ø¶Ø§ÙØ© Ø£Ù‚Ù†Ø¹Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
            for _ in range(len(batch_slices)):
                all_masks.append(np.zeros((128, 128, 1), dtype=np.uint8))
    
    masks_array = np.array(all_masks)
    non_empty_count = np.sum([np.any(mask > 0) for mask in masks_array])
    print(f"âœ… Streaming segmentation completed: {non_empty_count}/{len(masks_array)} non-empty masks")
    
    return masks_array

def create_efficient_mock_masks(slices):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‚Ù†Ø¹Ø© ÙˆÙ‡Ù…ÙŠØ© ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
    print("Creating efficient mock masks...")
    binary_masks = []
    
    for i, slice_img in enumerate(slices):
        mask = np.zeros_like(slice_img.squeeze())
        
        # Ø¥Ø¶Ø§ÙØ© Ø¢ÙØ§Øª ÙˆÙ‡Ù…ÙŠØ© ÙÙŠ 10% Ù…Ù† Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ ÙÙ‚Ø·
        if i % 10 == 0 and i > len(slices) * 0.3 and i < len(slices) * 0.7:
            h, w = mask.shape
            num_lesions = np.random.randint(2, 6)
            for _ in range(num_lesions):
                x = np.random.randint(20, w-20)
                y = np.random.randint(20, h-20)
                radius = np.random.randint(3, 10)
                cv2.circle(mask, (x, y), radius, 1, -1)
        
        binary_masks.append(mask.astype(np.uint8))
    
    return np.array(binary_masks)

# =====================================================
# Efficient TDA Processing for Large Files
# =====================================================

def efficient_tda_feature_extraction(masks, num_features_expected=603, sample_rate=0.3):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª TDA ÙØ¹Ø§Ù„ Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
    print("Efficient TDA feature extraction for large files...")
    
    if not TDA_AVAILABLE:
        print("âš ï¸ Using geometric features only (TDA not available)")
        return efficient_geometric_features(masks, num_features_expected)

    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹ÙŠÙ†Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ø£Ù‚Ù†Ø¹Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø¨
    total_masks = len(masks)
    if total_masks > 100:
        sample_size = max(50, int(total_masks * sample_rate))
        sample_indices = np.random.choice(total_masks, sample_size, replace=False)
        sampled_masks = [masks[i] for i in sample_indices]
        print(f"ğŸ”¬ Sampling {sample_size} masks from {total_masks} for TDA analysis")
    else:
        sampled_masks = masks
        sample_indices = list(range(total_masks))

    features_all = []
    persistence = VietorisRipsPersistence(
        homology_dimensions=[0, 1],
        n_jobs=1,
        max_edge_length=1.5,
        collapse_edges=True
    )

    for i, mask in enumerate(sampled_masks):
        if i % 20 == 0:
            print(f"  Processing TDA sample {i}/{len(sampled_masks)}")

        mask_2d = mask.squeeze()
        non_zero_pixels = np.sum(mask_2d > 0)

        if non_zero_pixels < 20:
            feats = create_compact_zero_features(num_features_expected)
        else:
            try:
                binary_mask = (mask_2d > 0).astype(np.float64)
                
                # ØªÙ†Ø¸ÙŠÙ morphological Ù…Ø¨Ø³Ø·
                cleaned_mask = ndimage.binary_opening(binary_mask, structure=np.ones((2,2)))
                points = np.column_stack(np.where(cleaned_mask > 0))

                if len(points) > 15:
                    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Ù‚Ø§Ø·
                    points = points.astype(np.float64)
                    points[:, 0] = points[:, 0] / binary_mask.shape[0]
                    points[:, 1] = points[:, 1] / binary_mask.shape[1]
                    
                    # Ø¥Ø¶Ø§ÙØ© Ø¶ÙˆØ¶Ø§Ø¡ Ø·ÙÙŠÙØ©
                    points += np.random.normal(0, 0.0001, points.shape)

                    diagrams = persistence.fit_transform([points])
                    feats = extract_efficient_features(diagrams, cleaned_mask)
                else:
                    feats = create_compact_zero_features(num_features_expected)

            except Exception as e:
                feats = create_compact_zero_features(num_features_expected)

        # Ø¶Ø¨Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if len(feats) < num_features_expected:
            feats.extend([0.0] * (num_features_expected - len(feats)))
        elif len(feats) > num_features_expected:
            feats = feats[:num_features_expected]

        features_all.append(feats)

    # ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù‚Ù†Ø¹Ø©
    full_features = []
    feature_idx = 0
    for i in range(total_masks):
        if i in sample_indices:
            full_features.append(features_all[feature_idx])
            feature_idx += 1
        else:
            full_features.append(create_compact_zero_features(num_features_expected))

    features_array = np.array(full_features)
    print(f"âœ… Efficient TDA features extracted: {features_array.shape}")
    
    return features_array

def efficient_geometric_features(masks, num_features_expected):
    """Ù…ÙŠØ²Ø§Øª Ù‡Ù†Ø¯Ø³ÙŠØ© ÙØ¹Ø§Ù„Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""
    print("Using efficient geometric features...")
    features_all = []
    
    for i, mask in enumerate(masks):
        if i % 50 == 0:
            print(f"  Processing geometric features {i}/{len(masks)}")
            
        mask_2d = mask.squeeze()
        binary_mask = (mask_2d > 0).astype(np.float64)
        feats = extract_efficient_features([], binary_mask)
        
        if len(feats) < num_features_expected:
            feats.extend([0.0] * (num_features_expected - len(feats)))
        
        features_all.append(feats)
    
    return np.array(features_all)

def create_compact_zero_features(num_features):
    """Ù…ÙŠØ²Ø§Øª ØµÙØ±ÙŠØ© Ù…Ø¶ØºÙˆØ·Ø©"""
    return [0.0] * num_features

def extract_efficient_features(diagrams, binary_mask):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙŠØ²Ø§Øª ÙØ¹Ø§Ù„"""
    features = []

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ØªØ¨ÙˆÙ„ÙˆØ¬ÙŠØ©
    for dim, diagram in enumerate(diagrams):
        if len(diagram) > 0:
            lifetimes = diagram[:, 1] - diagram[:, 0]
            
            topological_feats = [
                np.sum(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.mean(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.std(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.max(lifetimes) if len(lifetimes) > 0 else 0.0,
                len(lifetimes),
            ]
            features.extend(topological_feats)
        else:
            features.extend([0.0] * 5)

    # Ù…ÙŠØ²Ø§Øª Ù‡Ù†Ø¯Ø³ÙŠØ© Ù…Ø¨Ø³Ø·Ø©
    if np.sum(binary_mask) > 0:
        try:
            labeled, num_components = ndimage.label(binary_mask)
            if num_components > 0:
                sizes = ndimage.sum(binary_mask, labeled, range(1, num_components + 1))
                
                geometric_feats = [
                    np.sum(binary_mask),
                    num_components,
                    np.max(sizes) if len(sizes) > 0 else 0.0,
                    np.mean(sizes) if len(sizes) > 0 else 0.0,
                ]
                features.extend(geometric_feats)
            else:
                features.extend([0.0] * 4)
        except:
            features.extend([0.0] * 4)
    else:
        features.extend([0.0] * 4)

    return features

# =====================================================
# Efficient Probability Calculation
# =====================================================

def calculate_efficient_ms_probability(positive_slices, avg_prob, max_prob, binary_masks, probabilities, total_slices):
    """Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© MS ÙØ¹Ø§Ù„ Ù„Ù„Ø°Ø§ÙƒØ±Ø©"""

    if positive_slices == 0:
        return 5.0

    # Ø¹ÙˆØ§Ù…Ù„ Ù…Ø¨Ø³Ø·Ø©
    slice_ratio = positive_slices / total_slices
    slice_factor = min(slice_ratio * 2, 0.9)  # ØªØ¨Ø³ÙŠØ· Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­

    # Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ù‚Ø©
    high_confidence_count = np.sum(probabilities > 0.7)
    confidence_ratio = high_confidence_count / max(positive_slices, 1)
    prob_factor = avg_prob * min(confidence_ratio, 1.0)

    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¢ÙØ§Øª
    distribution_factor = calculate_efficient_distribution_factor(binary_masks, probabilities)

    # Ø­Ø³Ø§Ø¨ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…Ø¨Ø³Ø·
    ms_probability = (
        slice_factor * 0.35 +
        prob_factor * 0.35 +
        distribution_factor * 0.3
    )

    # ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ©
    ms_probability = min(ms_probability * 100, 95.0)

    # ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¢ÙØ§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©
    if high_confidence_count >= 5:
        ms_probability = min(ms_probability + 10, 95.0)
    elif high_confidence_count >= 2:
        ms_probability = min(ms_probability + 5, 95.0)

    return max(ms_probability, 5.0)

def calculate_efficient_distribution_factor(binary_masks, probabilities):
    """Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¢ÙØ§Øª ÙØ¹Ø§Ù„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    total_volume = 0
    high_prob_lesions = 0
    
    for i, mask in enumerate(binary_masks):
        if probabilities[i] > 0.3:
            total_volume += np.sum(mask)
            if probabilities[i] > 0.7:
                high_prob_lesions += 1
    
    if high_prob_lesions == 0:
        return 0.1
    
    # Ø­Ø³Ø§Ø¨ Ù…Ø¨Ø³Ø· Ù„Ù„ØªÙˆØ²ÙŠØ¹
    if high_prob_lesions >= 10:
        return 0.9
    elif high_prob_lesions >= 5:
        return 0.7
    elif high_prob_lesions >= 2:
        return 0.5
    else:
        return 0.3

# =====================================================
# Efficient Classification
# =====================================================

def efficient_fallback_classification(binary_masks):
    """ØªØµÙ†ÙŠÙ Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙØ¹Ø§Ù„"""
    predictions = []
    probabilities = []
    
    for mask in binary_masks:
        volume = np.sum(mask)
        if volume > 50:
            prob = min(volume / 500.0, 1.0)
            predictions.append(1)
            probabilities.append(prob)
        else:
            predictions.append(0)
            probabilities.append(0.0)
    
    return np.array(predictions), np.array(probabilities)

# =====================================================
# Visualization Functions (Unchanged)
# =====================================================

def create_clean_medical_visualization(mri_slice, binary_mask, probability):
    """Create CLEAN medical visualization WITHOUT PROB/CONF text"""
    if mri_slice.dtype != np.uint8:
        mri_slice = (mri_slice * 255).astype(np.uint8)

    if len(mri_slice.shape) == 2:
        mri_rgb = np.stack([mri_slice] * 3, axis=-1)
    else:
        mri_rgb = mri_slice.copy()

    mri_bgr = cv2.cvtColor(mri_rgb, cv2.COLOR_RGB2BGR)

    if binary_mask.shape[:2] != mri_bgr.shape[:2]:
        binary_mask_resized = cv2.resize(binary_mask.squeeze(),
                                       (mri_bgr.shape[1], mri_bgr.shape[0]))
    else:
        binary_mask_resized = binary_mask.squeeze()

    # Create colored mask based on probability
    if probability > 0.7:
        color = (0, 0, 255)  # Red
        alpha = 0.6
    elif probability > 0.4:
        color = (0, 165, 255)  # Orange
        alpha = 0.5
    else:
        color = (0, 255, 255)  # Yellow
        alpha = 0.4

    # Apply mask overlay
    mask_indices = binary_mask_resized > 0
    if np.any(mask_indices):
        overlay = mri_bgr.copy()
        overlay[mask_indices] = color
        mri_bgr = cv2.addWeighted(overlay, alpha, mri_bgr, 1 - alpha, 0)

        contours, _ = cv2.findContours(binary_mask_resized.astype(np.uint8),
                                     cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mri_bgr, contours, -1, color, 1)

    result_image = cv2.cvtColor(mri_bgr, cv2.COLOR_BGR2RGB)
    return result_image

def get_representative_slices(slices, binary_masks, predictions, probabilities, num_slices=6):
    """Get representative slices efficiently"""
    positive_indices = np.where(predictions == 1)[0]

    if len(positive_indices) == 0:
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø¢ÙØ§ØªØŒ Ù†Ø£Ø®Ø° Ø´Ø±Ø§Ø¦Ø­ Ù…Ù† Ø§Ù„Ù…Ù†ØªØµÙ
        middle_start = len(slices) // 3
        middle_end = 2 * len(slices) // 3
        if middle_end - middle_start > num_slices:
            selected_indices = np.linspace(middle_start, middle_end, num_slices, dtype=int)
        else:
            selected_indices = range(middle_start, min(middle_start + num_slices, len(slices)))
    else:
        # Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ù…Ø¹ Ø§Ù„Ø¢ÙØ§Øª
        sorted_indices = positive_indices[np.argsort(probabilities[positive_indices])[::-1]]
        selected_indices = sorted_indices[:min(num_slices, len(sorted_indices))]

    representative_slices = []
    for idx in selected_indices:
        if idx < len(slices):
            slice_data = {
                'index': int(idx),
                'probability': float(probabilities[idx]) if idx < len(probabilities) else 0.1,
                'original_slice': slices[idx].squeeze(),
                'binary_mask': binary_masks[idx].squeeze() if idx < len(binary_masks) else np.zeros_like(slices[idx].squeeze())
            }
            representative_slices.append(slice_data)

    return representative_slices

# =====================================================
# Flask Routes - Optimized for Large Files
# =====================================================

@app.route('/')
def home():
    return render_template('index_advanced.html')

@app.route('/health')
def health():
    return jsonify({
        'status': 'healthy',
        'message': 'Advanced MS MRI Analysis Server with TDA - Optimized for Large Files',
        'models_loaded': unet_model is not None,
        'tda_available': TDA_AVAILABLE,
        'max_file_size': '500MB',
        'memory_optimized': True,
        'large_file_support': True
    })

@app.route('/predict', methods=['POST'])
def predict():
    start_time = time.time()
    
    try:
        print("ğŸ¯ Received analysis request for large file")

        if 'nii_file' not in request.files:
            return jsonify({'error': 'No file uploaded'}), 400

        file = request.files['nii_file']
        if file.filename == '':
            return jsonify({'error': 'No file selected'}), 400

        # ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
        file.seek(0, 2)
        file_size = file.tell()
        file.seek(0)
        
        print(f"ğŸ“ Processing file: {file.filename} ({file_size / (1024*1024):.2f} MB)")

        MAX_FILE_SIZE = 500 * 1024 * 1024
        if file_size > MAX_FILE_SIZE:
            return jsonify({
                'error': 'File too large', 
                'max_size': '500MB',
                'your_file_size': f'{file_size / (1024*1024):.2f}MB',
                'status': 'error'
            }), 400

        with tempfile.NamedTemporaryFile(delete=False, suffix='.nii') as temp_file:
            file.save(temp_file.name)
            temp_path = temp_file.name

        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ¨ÙŠØ± Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙØ¹Ø§Ù„Ø©
            img_data, sample_indices, total_slices = process_very_large_nii_file(temp_path)
            
            # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ¯Ø±Ø¬Ø©
            slices = incremental_preprocessing(img_data)
            print(f"ğŸ”§ Preprocessed {len(slices)} slices efficiently")
            
            # ØªØ­Ø±ÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            del img_data
            gc.collect()

            # Ø§Ù„ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù…ØªØ¯Ø±Ø¬Ø©
            binary_masks = streaming_unet_segmentation(slices)
            print(f"ğŸ“Š Segmentation completed: {len(binary_masks)} masks")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„
            print("ğŸ”¬ Efficient TDA feature extraction...")
            try:
                tda_features = efficient_tda_feature_extraction(binary_masks)
                analysis_method = "Efficient TDA + Geometric" if TDA_AVAILABLE else "Efficient Geometric"
                
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
                if rf_model is not None and scaler is not None:
                    try:
                        tda_features_scaled = scaler.transform(tda_features)
                        predictions = rf_model.predict(tda_features_scaled)
                        probabilities = rf_model.predict_proba(tda_features_scaled)[:, 1]
                        print("âœ… Efficient classification successful")
                    except Exception as e:
                        print(f"Model classification failed: {e}")
                        predictions, probabilities = efficient_fallback_classification(binary_masks)
                        analysis_method = "Efficient Geometric"
                else:
                    predictions, probabilities = efficient_fallback_classification(binary_masks)
                    analysis_method = "Efficient Geometric"

            except Exception as e:
                print(f"TDA extraction failed: {e}")
                predictions, probabilities = efficient_fallback_classification(binary_masks)
                analysis_method = "Basic Geometric Analysis"

            # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            positive_slices = int(np.sum(predictions))
            avg_prob = float(np.mean(probabilities))
            max_prob = float(np.max(probabilities))

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
            ms_probability = calculate_efficient_ms_probability(
                positive_slices, avg_prob, max_prob, binary_masks, probabilities, len(slices)
            )

            print(f"ğŸ” Analysis Results:")
            print(f"   - Positive slices: {positive_slices}/{len(slices)}")
            print(f"   - MS Probability: {ms_probability:.1f}%")
            print(f"   - Analysis Method: {analysis_method}")

            # Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ÙŠØ©
            representative_slices = get_representative_slices(
                slices, binary_masks, predictions, probabilities, 6
            )

            # Ø§Ù„ØªØµÙˆØ±Ø§Øª
            visualization_images = []
            for slice_data in representative_slices:
                viz_image = create_clean_medical_visualization(
                    slice_data['original_slice'],
                    slice_data['binary_mask'],
                    slice_data['probability']
                )

                pil_img = Image.fromarray(viz_image)
                buffered = io.BytesIO()
                pil_img.save(buffered, format="PNG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')

                visualization_images.append({
                    'slice_index': slice_data['index'],
                    'probability': slice_data['probability'],
                    'image': img_base64
                })

            # Ø§Ù„ØªØ´Ø®ÙŠØµ
            if ms_probability > 75:
                diagnosis = "ğŸŸ¥ HIGH PROBABILITY OF MULTIPLE SCLEROSIS"
                severity = "High"
            elif ms_probability > 55:
                diagnosis = "ğŸŸ¨ SUSPICIOUS FOR DEMYELINATING DISEASE"
                severity = "Moderate"
            elif ms_probability > 30:
                diagnosis = "ğŸŸ¦ POSSIBLE EARLY MS OR OTHER CONDITION"
                severity = "Low"
            else:
                diagnosis = "âœ… NO SIGNIFICANT LESIONS DETECTED"
                severity = "None"

            # ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processing_time = time.time() - start_time

            # Ø§Ù„Ø±Ø¯
            response_data = {
                'diagnosis': diagnosis,
                'ms_probability': float(round(ms_probability, 1)),
                'positive_slices': positive_slices,
                'total_slices': len(slices),
                'avg_probability': float(round(avg_prob, 3)),
                'max_probability': float(round(max_prob, 3)),
                'severity': severity,
                'has_lesions': bool(positive_slices > 0),
                'status': 'success',
                'message': 'Efficient large file analysis completed successfully',
                'visualizations': visualization_images,
                'detailed_analysis': {
                    'analysis_method': analysis_method,
                    'tda_available': TDA_AVAILABLE,
                    'file_size_processed': f'{file_size / (1024*1024):.2f}MB',
                    'processing_time_seconds': round(processing_time, 1),
                    'large_file_optimized': True,
                    'sampling_method': 'Strategic sampling used',
                    'slices_analyzed': f"{len(slices)} of {total_slices} total"
                }
            }

            print(f"âœ… Efficient analysis completed in {processing_time:.1f}s")

            return jsonify(response_data)

        except Exception as e:
            print(f"âŒ Analysis error: {e}")
            return jsonify({
                'error': 'Analysis failed', 
                'message': str(e),
                'status': 'error'
            }), 500

        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    except Exception as e:
        print(f"âŒ General prediction error: {e}")
        return jsonify({'error': f'Processing failed: {str(e)}', 'status': 'error'}), 500

# =====================================================
# Main Execution
# =====================================================

def create_app():
    if not os.path.exists('templates'):
        os.makedirs('templates')
    return app

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    
    print("=" * 60)
    print("ğŸš€ MS MRI ANALYSIS SERVER - OPTIMIZED FOR LARGE FILES")
    print("=" * 60)
    print(f"ğŸ“¡ Server: http://0.0.0.0:{port}")
    print(f"ğŸ” Health: http://0.0.0.0:{port}/health")
    print(f"ğŸ“ Max file size: 500MB")
    print(f"ğŸ§  AI Models: {'âœ… Loaded' if unet_model is not None else 'âš ï¸ Basic Mode'}")
    print(f"ğŸ”¬ TDA: {'âœ… Available' if TDA_AVAILABLE else 'âš ï¸ Geometric Only'}")
    print(f"ğŸ’¾ Large file support: âœ… Enabled")
    print(f"âš¡ Efficient processing: âœ… Streaming & sampling")

    app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
