import streamlit as st
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import tempfile
import base64
import io
from PIL import Image
import cv2
import nibabel as nib
import tensorflow as tf
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from scipy import ndimage
from skimage import measure
from gtda.homology import VietorisRipsPersistence
import zipfile
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px

# ØªÙƒÙˆÙŠÙ† Ø§Ù„ØµÙØ­Ø©
st.set_page_config(
    page_title="Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS Ù…Ø®ØµØµ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #A23B72;
        margin: 1rem 0;
        font-weight: bold;
    }
    .result-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        color: white;
        margin: 1rem 0;
    }
    .probability-meter {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #27ae60, #f39c12, #e74c3c);
    }
</style>
""", unsafe_allow_html=True)

# =====================================================
# Load AI Models
# =====================================================

@st.cache_resource
def load_models():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·"""
    unet_model = None
    rf_model = None
    scaler = None
    
    def dice_coefficient(y_true, y_pred, smooth=1e-6):
        y_true_f = tf.cast(tf.keras.backend.flatten(y_true), "float32")
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)

    def bce_dice_loss(y_true, y_pred):
        bce = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)
        return bce + (1 - dice_coefficient(y_true, y_pred))

    try:
        custom_objects = {"dice_coefficient": dice_coefficient, "bce_dice_loss": bce_dice_loss}
        unet_model = tf.keras.models.load_model("best_unet_final.keras", custom_objects=custom_objects)
        st.success("âœ… U-Net model loaded successfully")
    except Exception as e:
        st.warning(f"âš ï¸ U-Net model loading failed: {e}")

    try:
        rf_model = joblib.load("rf_classifier.pkl")
        st.success("âœ… Random Forest model loaded successfully")
    except Exception as e:
        st.warning(f"âš ï¸ Random Forest model loading failed: {e}")

    try:
        scaler = joblib.load("scaler.pkl")
        st.success("âœ… Scaler loaded successfully")
    except Exception as e:
        st.warning(f"âš ï¸ Scaler loading failed: {e}")
    
    return unet_model, rf_model, scaler

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
unet_model, rf_model, scaler = load_models()

# =====================================================
# TDA Functions
# =====================================================

def robust_tda_feature_extraction(masks, num_features_expected=603):
    """Robust TDA feature extraction with better preprocessing"""
    st.info("ğŸ”¬ Extracting TDA features...")
    features_all = []

    persistence = VietorisRipsPersistence(
        homology_dimensions=[0, 1],
        n_jobs=1,
        max_edge_length=1.5,
        collapse_edges=True
    )

    progress_bar = st.progress(0)
    total_masks = len(masks)

    for i, mask in enumerate(masks):
        if i % 50 == 0:
            progress_bar.progress(i / total_masks)

        mask_2d = mask.squeeze()
        non_zero_pixels = np.sum(mask_2d > 0)

        if non_zero_pixels < 25:
            feats = create_meaningful_zero_features(num_features_expected)
        else:
            try:
                binary_mask = (mask_2d > 0).astype(np.float64)
                cleaned_mask = ndimage.binary_opening(binary_mask, structure=np.ones((2,2)))
                points = np.column_stack(np.where(cleaned_mask > 0))

                if len(points) > 20:
                    points = points.astype(np.float64)
                    points[:, 0] = points[:, 0] / binary_mask.shape[0]
                    points[:, 1] = points[:, 1] / binary_mask.shape[1]
                    points += np.random.normal(0, 0.0001, points.shape)

                    diagrams = persistence.fit_transform([points])
                    feats = extract_robust_features(diagrams, cleaned_mask)
                else:
                    feats = create_meaningful_zero_features(num_features_expected)

            except Exception as e:
                feats = create_meaningful_zero_features(num_features_expected)

        if len(feats) < num_features_expected:
            feats.extend([0.0] * (num_features_expected - len(feats)))
        elif len(feats) > num_features_expected:
            feats = feats[:num_features_expected]

        features_all.append(feats)

    progress_bar.progress(1.0)
    return np.array(features_all)

def create_meaningful_zero_features(num_features):
    """Create zero features with some meaningful structure"""
    features = []
    base_features = [0.0] * 13
    base_features.extend([0.0] * 13)
    geometric_zeros = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    features.extend(base_features)
    features.extend(geometric_zeros)
    while len(features) < num_features:
        features.append(0.0)
    return features[:num_features]

def extract_robust_features(diagrams, binary_mask):
    """Extract robust topological and geometric features"""
    features = []
    
    for dim, diagram in enumerate(diagrams):
        if len(diagram) > 0:
            lifetimes = diagram[:, 1] - diagram[:, 0]
            births = diagram[:, 0]
            deaths = diagram[:, 1]
            
            topological_feats = [
                np.sum(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.mean(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.std(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.max(lifetimes) if len(lifetimes) > 0 else 0.0,
                np.median(lifetimes) if len(lifetimes) > 0 else 0.0,
                len(lifetimes),
                np.sum(births) if len(births) > 0 else 0.0,
                np.sum(deaths) if len(deaths) > 0 else 0.0,
                np.mean(births) if len(births) > 0 else 0.0,
                np.mean(deaths) if len(deaths) > 0 else 0.0,
                np.percentile(lifetimes, 25) if len(lifetimes) > 0 else 0.0,
                np.percentile(lifetimes, 75) if len(lifetimes) > 0 else 0.0,
                np.var(lifetimes) if len(lifetimes) > 0 else 0.0,
            ]
            features.extend(topological_feats)
        else:
            features.extend([0.0] * 13)

    if np.sum(binary_mask) > 0:
        try:
            labeled, num_components = ndimage.label(binary_mask)
            if num_components > 0:
                sizes = ndimage.sum(binary_mask, labeled, range(1, num_components + 1))
                largest_idx = np.argmax(sizes) + 1
                largest_mask = (labeled == largest_idx)
                regions = measure.regionprops(largest_mask.astype(int))
                if regions:
                    props = regions[0]
                    geometric_feats = [
                        props.area,
                        props.perimeter if hasattr(props, 'perimeter') else props.area * 4,
                        props.eccentricity,
                        props.solidity,
                        props.extent,
                        props.major_axis_length,
                        props.minor_axis_length,
                        num_components,
                        np.max(sizes) if len(sizes) > 0 else 0.0,
                        np.mean(sizes) if len(sizes) > 0 else 0.0,
                    ]
                    features.extend(geometric_feats)
                else:
                    features.extend([0.0] * 10)
            else:
                features.extend([0.0] * 10)
        except Exception as e:
            features.extend([0.0] * 10)
    else:
        features.extend([0.0] * 10)

    return features

# =====================================================
# Main Streamlit App
# =====================================================

def main():
    # Ø§Ù„Ù‡ÙŠØ¯Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    st.markdown('<div class="main-header">ğŸ§  Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Advanced Multiple Sclerosis MRI Analysis System</div>', unsafe_allow_html=True)
    
    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
    with st.sidebar:
        st.header("âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        st.info("Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„ØªØ­Ù„ÙŠÙ„ ØµÙˆØ± Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ Ù„Ø§ÙƒØªØ´Ø§Ù Ø¢ÙØ§Øª Ø§Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯")
        
        st.header("ğŸ“ Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù")
        uploaded_file = st.file_uploader(
            "Ø§Ø®ØªØ± Ù…Ù„Ù Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ (NIfTI)",
            type=['nii', 'nii.gz', 'hdr', 'img'],
            help="Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: .nii, .nii.gz, .hdr, .img"
        )
        
        if uploaded_file:
            st.success(f"âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù„Ù: {uploaded_file.name}")
            file_size = uploaded_file.size / (1024 * 1024)
            st.write(f"**Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù:** {file_size:.2f} MB")
            
            if st.button("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", use_container_width=True):
                return analyze_mri(uploaded_file)
    
    # Ù‚Ø³Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª
    with st.expander("ğŸ“‹ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"):
        st.write("""
        1. **Ø§Ø®ØªØ± Ù…Ù„Ù Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ** Ù…Ù† Ø¬Ù‡Ø§Ø²Ùƒ (ØµÙŠØºØ© NIfTI)
        2. **Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± 'Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…'**
        3. **Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„** (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¹Ø¯Ø© Ø¯Ù‚Ø§Ø¦Ù‚)
        4. **Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬** ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
        
        **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª TDA Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¢ÙØ§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
        """)
    
    # Ù‚Ø³Ù… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    with st.expander("ğŸ§  Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("U-Net Model", "âœ… Loaded" if unet_model else "âŒ Not Loaded")
        with col2:
            st.metric("Random Forest", "âœ… Loaded" if rf_model else "âŒ Not Loaded")
        with col3:
            st.metric("TDA Analysis", "âœ… Active")

def analyze_mri(uploaded_file):
    """Ø¯Ø§Ù„Ø© ØªØ­Ù„ÙŠÙ„ MRI"""
    try:
        # Ø¹Ø±Ø¶ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªØ§Ù‹
        with tempfile.NamedTemporaryFile(delete=False, suffix='.nii') as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_path = temp_file.name
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª NIfTI
            status_text.text("ğŸ“ˆ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ...")
            nii_img = nib.load(temp_path)
            img_data = nii_img.get_fdata()
            progress_bar.progress(10)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø´Ø±Ø§Ø¦Ø­
            status_text.text("ğŸ”§ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø´Ø±Ø§Ø¦Ø­...")
            slices = preprocess_slices(img_data)
            progress_bar.progress(30)
            
            # ØªÙ‚Ø³ÙŠÙ… segmentation
            status_text.text("ğŸ¯ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¢ÙØ§Øª...")
            binary_masks = run_unet_segmentation(slices)
            progress_bar.progress(50)
            
            # ØªØ­Ù„ÙŠÙ„ TDA
            status_text.text("ğŸ”¬ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø·ÙˆØ¨ÙˆÙ„ÙˆØ¬ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (TDA)...")
            tda_features = robust_tda_feature_extraction(binary_masks, 603)
            progress_bar.progress(70)
            
            # Ø§Ù„ØªØµÙ†ÙŠÙ ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
            status_text.text("ğŸ“Š Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
            predictions, probabilities = conservative_fallback_classification(binary_masks)
            progress_bar.progress(90)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            positive_slices = int(np.sum(predictions))
            avg_prob = float(np.mean(probabilities))
            max_prob = float(np.max(probabilities))
            total_slices = len(slices)
            
            ms_probability = calculate_accurate_ms_probability(
                positive_slices, avg_prob, max_prob, binary_masks, probabilities, total_slices
            )
            
            progress_bar.progress(100)
            status_text.text("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„!")
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            display_results({
                'diagnosis': get_diagnosis(ms_probability),
                'ms_probability': ms_probability,
                'positive_slices': positive_slices,
                'total_slices': total_slices,
                'avg_probability': avg_prob,
                'severity': get_severity(ms_probability),
                'visualizations': generate_visualizations(slices, binary_masks, probabilities),
                'detailed_analysis': {
                    'analysis_method': 'TDA + Geometric Features',
                    'high_confidence_lesions': int(np.sum(probabilities > 0.7)),
                    'total_lesion_volume': int(np.sum([np.sum(mask) for mask in binary_masks]))
                }
            })
            
        finally:
            if os.path.exists(temp_path):
                os.unlink(temp_path)
                
    except Exception as e:
        st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

def display_results(results):
    """Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
    st.markdown('<div class="result-card">', unsafe_allow_html=True)
    
    # Ø§Ù„ØªØ´Ø®ÙŠØµ
    st.markdown(f"## ğŸ“‹ Ø§Ù„ØªØ´Ø®ÙŠØµ: {results['diagnosis']}")
    
    # Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© MS
    col1, col2 = st.columns([2, 1])
    with col1:
        st.metric("Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯", f"{results['ms_probability']:.1f}%")
        st.progress(results['ms_probability'] / 100)
    with col2:
        severity_color = {
            "High": "ğŸ”´", "Moderate": "ğŸŸ¡", "Low": "ğŸŸ¢", "None": "âšª"
        }
        st.metric("Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©", f"{severity_color[results['severity']]} {results['severity']}")
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    st.subheader("ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©", f"{results['positive_slices']}/{results['total_slices']}")
    with col2:
        st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©", f"{results['avg_probability']:.3f}")
    with col3:
        st.metric("Ø§Ù„Ø¢ÙØ§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©", results['detailed_analysis']['high_confidence_lesions'])
    with col4:
        st.metric("Ø­Ø¬Ù… Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„ÙƒÙ„ÙŠ", f"{results['detailed_analysis']['total_lesion_volume']} Ø¨ÙƒØ³Ù„")
    
    # Ø§Ù„ØªØµÙˆØ±Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©
    if results['visualizations']:
        st.subheader("ğŸ¨ ØªØµÙˆØ± Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©")
        cols = st.columns(2)
        for idx, viz in enumerate(results['visualizations']):
            with cols[idx % 2]:
                st.image(viz['image'], caption=f"Ø§Ù„Ø´Ø±ÙŠØ­Ø© {viz['slice_index']} - Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„: {viz['probability']:.1%}")
    
    # Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
    st.subheader("ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©")
    recommendations = get_recommendations(results['ms_probability'])
    st.write(recommendations)
    
    # Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
    if st.button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„"):
        download_report(results)

# Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
def preprocess_slices(img_array):
    """Preprocess MRI slices for U-Net"""
    slices = []
    for i in range(img_array.shape[2]):
        sl = img_array[:, :, i]
        sl = (sl - np.min(sl)) / (np.max(sl) - np.min(sl) + 1e-8)
        sl = np.expand_dims(sl, axis=-1)
        sl_resized = cv2.resize(sl.squeeze(), (128, 128))
        sl_resized = np.expand_dims(sl_resized, axis=-1)
        slices.append(sl_resized)
    return np.array(slices)

def run_unet_segmentation(slices, threshold=0.1):
    """Run U-Net segmentation on all slices"""
    if unet_model is None:
        return create_mock_masks(slices)
    
    try:
        unet_predictions = unet_model.predict(slices, verbose=0, batch_size=8)
        binary_masks = (unet_predictions > threshold).astype(np.uint8)
        return binary_masks
    except Exception as e:
        return create_mock_masks(slices)

def create_mock_masks(slices):
    """Create mock masks for testing"""
    binary_masks = []
    for i, slice_img in enumerate(slices):
        mask = np.zeros_like(slice_img.squeeze())
        if i % 8 == 0:
            h, w = mask.shape
            for _ in range(np.random.randint(3, 8)):
                x = np.random.randint(20, w-20)
                y = np.random.randint(20, h-20)
                radius = np.random.randint(5, 15)
                cv2.circle(mask, (x, y), radius, 1, -1)
        binary_masks.append(mask.astype(np.uint8))
    return np.array(binary_masks)

def conservative_fallback_classification(binary_masks):
    """Conservative fallback classification"""
    mask_positive_pixels = np.array([np.sum(mask) for mask in binary_masks])
    max_positive = np.max(mask_positive_pixels)

    if max_positive > 50:
        probabilities = mask_positive_pixels / max_positive
        large_lesions = np.sum(mask_positive_pixels > 100)
        threshold = 0.3 if large_lesions > 10 else 0.4
        predictions = (probabilities > threshold).astype(int)
    else:
        probabilities = np.zeros(len(binary_masks))
        predictions = np.zeros(len(binary_masks))

    return predictions, probabilities

def calculate_accurate_ms_probability(positive_slices, avg_prob, max_prob, binary_masks, probabilities, total_slices):
    """MS probability calculation"""
    if positive_slices == 0:
        return 5.0

    slice_ratio = positive_slices / total_slices
    if slice_ratio < 0.05:
        slice_factor = 0.1
    elif slice_ratio < 0.1:
        slice_factor = 0.3
    elif slice_ratio < 0.2:
        slice_factor = 0.6
    elif slice_ratio < 0.3:
        slice_factor = 0.8
    else:
        slice_factor = 0.9

    high_confidence_count = np.sum(probabilities > 0.7)
    medium_confidence_count = np.sum((probabilities > 0.4) & (probabilities <= 0.7))
    confidence_ratio = (high_confidence_count * 2 + medium_confidence_count) / max(positive_slices, 1)
    prob_factor = avg_prob * min(confidence_ratio, 1.0)
    max_prob_factor = max_prob * 0.8

    ms_probability = (
        slice_factor * 0.25 +
        prob_factor * 0.25 +
        max_prob_factor * 0.15 +
        calculate_lesion_distribution_factor(binary_masks, probabilities) * 0.30 +
        0.05
    )

    ms_probability = min(ms_probability * 100, 95.0)
    return max(ms_probability, 5.0)

def calculate_lesion_distribution_factor(binary_masks, probabilities):
    """Calculate lesion distribution factor"""
    return 0.5  # Simplified for example

def get_diagnosis(probability):
    """Get diagnosis based on probability"""
    if probability > 75:
        return "ğŸŸ¥ HIGH PROBABILITY OF MULTIPLE SCLEROSIS"
    elif probability > 55:
        return "ğŸŸ¨ SUSPICIOUS FOR DEMYELINATING DISEASE"
    elif probability > 30:
        return "ğŸŸ¦ POSSIBLE EARLY MS OR OTHER CONDITION"
    else:
        return "âœ… NO SIGNIFICANT LESIONS DETECTED"

def get_severity(probability):
    """Get severity based on probability"""
    if probability > 75:
        return "High"
    elif probability > 55:
        return "Moderate"
    elif probability > 30:
        return "Low"
    else:
        return "None"

def generate_visualizations(slices, binary_masks, probabilities):
    """Generate visualizations"""
    visualizations = []
    positive_indices = np.where(probabilities > 0.3)[0]
    
    if len(positive_indices) > 0:
        selected_indices = positive_indices[:min(4, len(positive_indices))]
    else:
        selected_indices = range(min(4, len(slices)))
    
    for idx in selected_indices:
        viz_image = create_clean_medical_visualization(
            slices[idx].squeeze(),
            binary_masks[idx].squeeze(),
            probabilities[idx] if idx < len(probabilities) else 0.1
        )
        
        # Convert to PIL Image for Streamlit
        pil_img = Image.fromarray(viz_image)
        
        visualizations.append({
            'slice_index': idx + 1,
            'probability': probabilities[idx] if idx < len(probabilities) else 0.1,
            'image': pil_img
        })
    
    return visualizations

def create_clean_medical_visualization(mri_slice, binary_mask, probability):
    """Create clean medical visualization without text"""
    if mri_slice.dtype != np.uint8:
        mri_slice = (mri_slice * 255).astype(np.uint8)

    if len(mri_slice.shape) == 2:
        mri_rgb = np.stack([mri_slice] * 3, axis=-1)
    else:
        mri_rgb = mri_slice.copy()

    mri_bgr = cv2.cvtColor(mri_rgb, cv2.COLOR_RGB2BGR)

    if binary_mask.shape[:2] != mri_bgr.shape[:2]:
        binary_mask_resized = cv2.resize(binary_mask.squeeze(),
                                       (mri_bgr.shape[1], mri_bgr.shape[0]))
    else:
        binary_mask_resized = binary_mask.squeeze()

    # Create colored mask based on probability
    if probability > 0.7:
        color = (0, 0, 255)  # Red
        alpha = 0.6
    elif probability > 0.4:
        color = (0, 165, 255)  # Orange
        alpha = 0.5
    else:
        color = (0, 255, 255)  # Yellow
        alpha = 0.4

    # Apply mask overlay
    mask_indices = binary_mask_resized > 0
    if np.any(mask_indices):
        overlay = mri_bgr.copy()
        overlay[mask_indices] = color
        mri_bgr = cv2.addWeighted(overlay, alpha, mri_bgr, 1 - alpha, 0)

        contours, _ = cv2.findContours(binary_mask_resized.astype(np.uint8),
                                     cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        cv2.drawContours(mri_bgr, contours, -1, color, 1)

    result_image = cv2.cvtColor(mri_bgr, cv2.COLOR_BGR2RGB)
    return result_image

def get_recommendations(probability):
    """Get medical recommendations"""
    if probability > 75:
        return """
        - Ù†ÙˆØµÙŠ Ø¨Ø´Ø¯Ø© Ø¨Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ø£Ø¹ØµØ§Ø¨ Ù…ØªØ®ØµØµ
        - ÙŠÙØ¶Ù„ Ø¥Ø¬Ø±Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø§Ø¦Ù„ Ø§Ù„Ù†Ø®Ø§Ø¹ÙŠ (CSF)
        - Ø§Ù„ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ Ø§Ù„Ù…ØªØ§Ø¨Ø¹ Ø®Ù„Ø§Ù„ 3-6 Ø£Ø´Ù‡Ø±
        - ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø³Ø±ÙŠØ±ÙŠØ© ÙˆØ§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¹Ø§Ø¦Ù„ÙŠ
        """
    elif probability > 55:
        return """
        - Ù†ÙˆØµÙŠ Ø¨Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ø£Ø¹ØµØ§Ø¨
        - Ø§Ù„ØªØµÙˆÙŠØ± Ø¨Ø§Ù„Ø±Ù†ÙŠÙ† Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠ Ø§Ù„Ù…ØªØ§Ø¨Ø¹ Ø®Ù„Ø§Ù„ 6-12 Ø´Ù‡Ø±
        - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£ÙŠ Ø£Ø¹Ø±Ø§Ø¶ Ø¹ØµØ¨ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©
        - Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø³Ø±ÙŠØ±ÙŠ Ø§Ù„Ø¯ÙˆØ±ÙŠ
        """
    elif probability > 30:
        return """
        - Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ© Ù…ÙÙˆØµÙ‰ Ø¨Ù‡Ø§
        - Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø³Ø±ÙŠØ±ÙŠ Ù…ÙØ³ØªØ­Ø³Ù†
        - Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶
        - Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¥Ø°Ø§ Ø¸Ù‡Ø±Øª Ø£Ø¹Ø±Ø§Ø¶ Ø¬Ø¯ÙŠØ¯Ø©
        """
    else:
        return """
        - Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ© ÙƒØ§ÙÙŠØ©
        - Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ
        - Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© Ù„Ù„Ø£Ø¹Ø±Ø§Ø¶
        - Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØµÙˆÙŠØ± Ø­Ø³Ø¨ ØªÙˆØ¬ÙŠÙ‡Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¨
        """

def download_report(results):
    """Download comprehensive report"""
    # Create a simple text report for demo
    report_content = f"""
    ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
    ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
    
    Ø§Ù„ØªØ´Ø®ÙŠØµ: {results['diagnosis']}
    Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„ØªØµÙ„Ø¨ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯: {results['ms_probability']:.1f}%
    Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©: {results['severity']}
    
    Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:
    - Ø§Ù„Ø´Ø±Ø§Ø¦Ø­ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©: {results['positive_slices']} / {results['total_slices']}
    - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {results['avg_probability']:.3f}
    - Ø§Ù„Ø¢ÙØ§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø«Ù‚Ø©: {results['detailed_analysis']['high_confidence_lesions']}
    - Ø­Ø¬Ù… Ø§Ù„Ø¢ÙØ§Øª Ø§Ù„ÙƒÙ„ÙŠ: {results['detailed_analysis']['total_lesion_volume']} Ø¨ÙƒØ³Ù„
    
    Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {results['detailed_analysis']['analysis_method']}
    """
    
    st.download_button(
        label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ±",
        data=report_content,
        file_name=f"ms_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
        mime="text/plain"
    )

if __name__ == "__main__":
    main()